## MaskFormer：每个像素的分类并不是语义分割所需要的全部

### 0.摘要

现代方法通常将语义分割定义为每个像素的分类任务，而实例级分割则采用另一种掩码分类来处理。我们的关键见解是：掩模分类足够通用，可以使用完全相同的模型、损失和训练过程，以统一的方式解决语义级和实例级的分割任务。据此，我们提出了**`MaskFormer`**，一个简单的掩模分类模型，它预测一组二进制掩模，每个掩模与单个全局类标签预测相关联。总的来说，所提出的基于掩模分类的方法简化了语义和全景分割任务的有效方法，并显示了良好的经验结果。特别是，我们观察到，当类的数量较大时，`MaskFormer`优于每像素的分类基线。我们的基于掩模分类的方法优于目前最先进的语义分割(`ADE20K`上的`55.6mIoU`)和全景分割(`COCO`上的`52.7PQ`)模型。

### 1.介绍

语义分割的目的是将图像划分为具有不同语义类别的区域。从`Longetal.`的全卷积网络(`FCNs`)工作开始，大多数基于深度学习的语义分割方法将语义分割定义为每像素分类（图1左），对每个输出像素应用分类损失。在这个公式中，每像素的预测很自然地将图像划分为不同类别的区域。

![image-20220318115500608](MaskFormer：每个像素的分类并不是语义分割所需要的全部.assets/image-20220318115500608.png)

掩模分类是一种分离图像分割和分割的分类范例。基于掩码分类的方法预测二值掩码，而是预测一组二值掩码，每个掩码与单个类预测相关联（图1右）。更灵活的掩模分类在实例级分割领域中占主导地位。`MaskR-CNN`[21]和`DETR`[4]都对每个片段产生单一类预测，例如全景分割。相比之下，每个像素的分类假设有一个静态数量的输出，并且不能返回一个可变数量的预测区域/段，而这是实例级任务所必需的。

我们的关键观察是：掩码分类足够通用，可以解决语义和实例分割任务。事实上，在`FCN`[30]之前，表现最好的语义分割方法，如`O2P`[5]和`SDS`[20]，使用了掩码分类公式。从这个角度来看，一个很自然的问题出现了：一个单一的掩码分类模型能否是简化语义级和实例级分割任务的有效方法？这种掩码分类模型在语义分割方面能否优于现有的每像素分类方法？

为了解决这两个问题，我们提出了一种简单的`MaskFormer`方法，该方法可以无缝地将任何现有的每像素分类模型转换为掩模分类。利用`DETR`[4]中提出的集合预测机制，`MaskFormer`使用`Transformer`解码器[41]来计算一组对，每个方法都由一个类预测和一个掩模嵌入向量组成。利用掩模嵌入向量，通过点积和每像素嵌入得到二进制掩模预测。新的模型以统一的方式解决了语义级和实例级的分割任务：不需要改变模型、损失和训练过程。具体来说，对于语义和全景分割任务，使用相同的每像素二进制掩模损失和每个掩模的单一分类损失。最后，我们设计了一个简单的推理策略，将面具前的输出混合到一个任务依赖的预测格式中。

我们在5个不同类别的语义分割数据集上评估了模型：城市景观[15]（19类）、[34]（65类）、ADE20K[55]（150个类）、COCOCOStuff-10K[3]（171类）和ADE20K-Full[55]（847个类）。虽然MaskFrorm与城市景观的每像素分类模型相当，该模型有一些不同的类，但新模型在词汇量更大的数据集上表现出了优越的性能。我们假设每个掩模模型的单一类预测的细粒度识别优于每个像素的类预测。MaskFromar实现了ADE20K的新技术(55.6mIoU)，比具有相同主干的每像素分类模型[29]高出2.1mIoU，同时效率更高（参数减少10%，流量减少40%）。

最后，我们研究了使用COCO[28,24]和ADE20K[55]两个全视分割数据集：COCO[28,24]解决实例级任务的能力。具有相同主干和相同后处理的面具模型优于更复杂的DETR模型[4]。此外，MaskFormer在COCO上实现了最先进的COCO(52.7PQ)，比之前的最先进的[42]多出1.6PQ。我们的实验强调了面具前者统一实例和语义级分割的能力。

### 2.相关工作

每像素分类和掩模分类在语义分割中都得到了广泛的研究。在早期的工作中，小岸和Yuille[25]应用了基于局部图像统计的每像素贝叶斯分类器。然后，受早期非语义分组[13,36]研究的启发，基于掩码分类的方法开始流行起来，证明了在帕斯卡VOC挑战[18]中的最佳性能。O2P[5]和CFM[16]等方法通过对掩模建议[6,40,2]进行分类，取得了最先进的结果。2015年，FCN[30]将每像素分类的思想扩展到深度网，在mIoU（每像素的每像素分类度量）上显著优于之前的所有方法。

自全卷积网络(FCNs)[30]的开创性工作开始以来，每像素分类就成为了基于深度网络的语义分割的主导方式。现代语义分割模型侧重于在最终的特征图中聚合长期上下文：ASPP[7,8]使用不同无效速率的无效卷积；PPM[52]使用不同内核大小的池操作符；DANet[19]、OCNet[51]和CCNet[23]使用非本地块[43]的不同变体。最近，SETR[53]和分段器[37]用视觉Transformer(ViT)[17]取代了传统的卷积骨干，它可以从第一层开始捕获远程上下文。然而，这些一致的基于Transformer的[41]语义分割方法仍然使用每像素分类公式。注意，我们的MaskFrare模块可以将任何每像素分类模型转换为掩码分类设置，允许无缝采用每像素分类的进展。

掩码分类通常用于实例级分割任务[20,24]。这些任务需要动态数量的预测，这使得每像素分类的应用具有挑战性，因为它假定一个静态的输出数量。著名的MaskR-CNN[21]使用全局分类器对掩码建议进行实例分割。DETR[4]进一步结合了一个Transformer[41]设计，以同时处理事物和东西分割的全视分割[24]。然而，这些掩码分类方法需要对边界框进行预测，这可能会限制它们在语义分割中的应用。最近提出的Max-DeepLab[42]消除了条件卷积[39,44]对全景分割的边界框预测的依赖。然而，除了主要的掩码分类损失外，它还需要多个辅助损失(即实例识别损失、掩码-id交叉熵损失和标准的每像素分类损失)。

### 3.从每个像素到掩码的分类

在本节中，我们首先描述如何将语义分割表述为每像素分类或掩码分类问题。然后，我们在Transformer解码器[41]的帮助下，引入了掩模分类模型的实例化。最后，我们描述了一种简单的推理策略，它可以将掩码分类输出转换为任务依赖的预测格式。

#### 3.1 每像素的分类公式

