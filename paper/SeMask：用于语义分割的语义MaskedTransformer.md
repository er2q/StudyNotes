## SeMask：用于语义分割的语义Masked Transformer

> （SeMask: Semantically Masked Transformers for Semantic Segmentation）个人理解：将语义地Masked Transformer信息用于语义分割。

### 摘要

在图像`transformer`网络的解码部分，对预训练主干结构进行微调一直是语义分割任务的传统方法。然而，**这种方法省略了图像在编码阶段提供的语义上下文信息。**本文论述，将语义信息整合到预训练分层级的`transformer-base`主干上，同时对其进行微调，可以显著提高信息。为了实现这个，我们提出了`SeMask`，一种简单有效的框架，借助于语义注意力操作，将语义信息整合到编码器中。另外，在训练中，我们使用了一个轻量级的语义解码器，在每个阶段对中间语义先验映射进行监督。我们的实验表明，语义先验的加入提高了所建立的层次编码器的性能，增加了少量`FLOPs`。我们通过将`SeMask`集成到每个变体的`Swin-Transformer`中，作为与不同解码器配对的编码器，提供了经验证明。我们的框架在`ADE20K`数据集上达到了`58.22% mIoU`的最新技术水平和在`Cityscapes`数据集上`mIoU`指标的改进超过`3%`。代码和模型公开在：[https://github.com/PicsartAI-Research/SeMask-Segmentation](t https://github.com/PicsartAI-Research/SeMask-Segmentation)。

### 1.介绍

语义分割旨在执行密集预测，以标记图像中的每个像素，该像素对应于改像素所代表的类。基于`transformer`的视觉网络在图像分类任务中的表现优于卷积神经网络。在现代，当转移到语义分割等下游任务时，`transformer`骨干已经显示出令人印象深刻的性能。

视觉'transformer'中的大多数框架设计都以两种方式之一来解决问题：(1)使用现有的预先训练的主干作为编码器，并使用现有的标准解码器(如`Semantic FPN`或`UperNet`)将其传输到下游任务；或者(2)设计一种新的编码器-解码器网络，在`ImageNet`上对编码器进行预处理，完成语义分割任务。正如前面提到的，这两种方法都涉及到对分割任务的编码器骨干进行微调。从大规模数据集进行微调，有助于早期的注意层在`transformer`的较低层合并局部信息。然而，由于数据集相对较小，并且从分类任务到分割任务，语义类的数量和性质发生了变化，因此在精细调优过程中仍然不能利用语义上下文信息。分层视觉`transformer`解决了沿着阶段逐级下下采样特征的问题，尽管它们仍然缺乏图像的语义上下文。

`Liu`等人介绍了`Swin Transformer`，它构建了分层特征图，使其与主要下游视觉任务的通用骨干兼容。建议使用两个注意力：全局下采样和局部下采样在`PVT`和`CPVT`有效分割。`Xie`等人进一步改进了分层`transformer`编码器，使其不受位置编码的影响，从而对分割任务中通常发现的不同分辨率具有鲁棒性。所有这些工作都对编码器进行了改进，使其更好地工作于下游任务，如分割，并取得了令人印象深刻的成功。然而，他们并没有注意捕捉整个图像的语义级上下文信息。缺乏语义级上下文信息会导致次优的分割性能，特别是在小对象的情况下，这些对象与较大类别的边界合并，导致错误的预测。最近，有人试图通过设计一个纯基于`transformer`的解码器来解决这个问题，该解码器将`patch`和类嵌入联合处理。然而，当与`Swin`和`Twins Transformers`等主要变压器骨干一起使用时，它对于微小的变体并不能有效地执行，并且由于分层架构导致次优性能而失败。

`Jin`等人，提出`ISNet`通过在解码器结构中引入`SLCM`和`ILCM`模块来对图像级上下文信息和语义级上下文信息进行建模。但是仍然有一点需要注意：`ISNet`是一种基于`CNN`的方法，只关注网络的解码器部分，而没有改变编码器。

![image-20220214230159992](.\SeMask：用于语义分割的语义MaskedTransformer\image-20220214230159992.png)

为解决上述问题，我们提出`SeMask`框架，将语义信息整合到层次视觉`transformer`体系结构中，并通过语义上下文增强`transformer`捕获的全局特征信息。现有框架将该体系结构描述为一个编码器-解码器结构，其中在`ImageNet`上预先训练的`transformer`充当编码器，并使用专门的解码器进行语义分割。与直接使用分层`transformer`作为主干相比，我们在主干的每个阶段的`transformer`层之后插入一个语义层，从而得到主干的`SeMask`版本，如图1所示。在图1中，我们展示了提供语义先验如何帮助改进最终的分割图。我们使用轻量级语义解码器来累积来自所有阶段的语义图，并使用像`Semantic-FPN`这样的标准解码器来进行主要的逐像素预测。**在整个编码器中添加语义建模和特征建模，有助于提高语义分割任务的性能。**在第4节中，我们将提出的`SeMask`块`Semantic-FPN`和`MaskFormer`解码器集成到`Swin Transformer`中，并使用四种不同的`transformer`变体进行实验：`Tiny、Small、Base和Large`。我们的实验结果表明，在两个不同的数据集上，所有的变形在语义分割上有相当大的改进。总之，我们的贡献有三点：

- 据我们所知，我们是第一个研究添加语义上下文到预先训练的`transformer`主干中用于语义分割的效果。此外，我们引入了`SeMask Block`模块，该模块可以插入任何现有的分层视觉`transformer`。
- 我们也建议使用一个简单的语义解码器来聚合来自编码器不同阶段的语义先验。使用每像素交叉熵损失的`ground true`分割标签对语义先验进行监督。
- 最后，我们深入的分析`SeMask Block`对不同数据集`ADE20k和Cityscenes`的影响。我们在`ADE20k`数据集上实现了新的最先进的性能，并在`Cityscenes`数据集上使用`SeMask Swin-Tiny+FPN`框架实现了3%以上的改进。

### 2.相关工作

#### 2.1.语义分割

语义分割广义上形成了密集的逐像素分类任务。`FCN`的开创性工作介绍了深度`CNN` 的使用，去除完全连接的层来处理分割任务。接下来的几部作品都基于同样的思想，即使用编码器-解码器结构。引入了在`DCNN`中使用`astrous`卷积来解决信号下采样问题。后来，各方面的工作都集中在最终的`feature map`中聚合的远程上下文上：`ASPP`使用不同膨胀率的空洞卷积；`PPM`使用不同内核大小的池化算子。

最近基于`DCNN`的模型专注于有效地聚合来自预训练的基于主干的编码器的分层特征，并带有专门设计的模块：在解码器中引入注意模块；使用不同形式的非局部模块；提出了一种新颖的`FAM`模块来解决使用语义流的错位问题；`AlignSeg`提出了对齐特征聚合模块和对齐上下文建模模块，以使上下文特征更好地对齐。使用分割架来获得更好的信息流。在这项工作中，我们还按照既定方向使用预训练的主干，并使用`Semantic-FPN`解码器聚合分层特征。

#### 2.2.使用`Transformer`的分割

在自然语言处理领域被大量使用后，基于`transformer`的模型自从引入`ViT`用于图像分类以来，已经在各种计算机视觉任务中广受欢迎。`SETR`使用`ViT`作为编码器和两个基于渐进上采样和多级特征聚合的解码器。`SegFormer`提出使用分层金字塔视觉`transformer`网络作为编码器和基于`MLP`的解码器来获得分割掩码。`Segmenter`将掩码转化器设计为解码器，它使用可学习的类映射标记来提高解码性能。`MaskFormer`从掩模分类的角度定义了每像素分类的问题，为所有分割任务创建了一个一体化模块。`Mask2Former`进一步发展了掩蔽注意力，以在一个框架中解决全景、实例和语义分割任务。最新的基于`transformer`的分割框架是基于微调预训练的分层主干作为解码器，以及像`Semantic-FPN`和`UperNet`这样的标准解码器来完成分割任务。在这项工作中，我们遵循相同的范式，此外，提出了一个框架来增强预训练的视觉`transformer`主干的微调能力。请注意，最近还有像`SwinV2`这样的并行工作，通过使用改进的巨型主干（例如具有30亿个参数的`SwinV2-G`），在`ADE20k`基准上达到了新的最先进性能。这超出了这项工作的范围，我们遵循主要基于`Swin-L`主干的当前实践。从理论上讲，如果我们将我们的方法应用于此类巨型模型，我们可以获得更好的性能。

#### 2.3.分割中的语义上下文

`zhang`等人，提出了上下文编码模块，它捕获全局语义上下文以及反馈循环，以平衡`ResNet`主干提取的特征中类的重要性。最近，专注于使用专门设计的解码器捕获和集成语义级上下文信息以及图像级上下文，这显示了基于`DCNN`的方法德 显著改进。这些作品中的每一个都基于提取的特征而不是编码器捕获语义特征的能力在编码器阶段之后捕获语义上下文。

在这项工作中，我们认为语义信息在编码阶段会丢失，因此，提出了一个框架来捕获语义信息，该框架可以插入任何预训练的视觉`transformer`骨干网络。

### 3.方法

